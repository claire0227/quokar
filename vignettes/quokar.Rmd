---
title: "Quokar Vignette"
author: "Wenjing Wang"
date: "11/8/2016"
output: html_document
---


## Outlier diagnostics of quantile regression in R: a vignette

### Abstract

Outlier diagnostics is an important aspect of statistic
inference. The interference effects or extra information provided by
outliers and their diagnostic methods are recently being studied in
quantile regression model. An implementation of these methods in R
language is available in the package `quokar`. This vignette offers a
brief tutorial introduction to the package. Package `quokar` is
open-source and can be freely downloaded from Github:
http://www.github.com/wenjingwang/quokar.

### Introduction

Koenker et al(1978) extended the OLS regression by introducing
quantile regression into linear model. In econometrics, social
sciences and ecology field, quantile regression methods have been
widely used to study wage determination, and income inequality.
There are several reasons to use quantile regression: (1)it relexed
the assumptions of classical regression model; (2)quantile regressions
are based on every quantiles, they are robust when observations have
outliers; (3)estimators of quantile regression have good large sample
asymptotic properties.

The distribution function of random variable $Y$ can be characterized
as

$$F(y)=P\{Y \leq y \} \enspace \enspace (1)$$

For any $0<\tau<1$,

$$Q(\tau)=\inf \{y:F(y)\geq \tau\} \enspace \enspace (2)$$

is called the $\tau$th quantile of $Y$. The quantile function provides
a complete characterization of $Y$.

OLS regression(also called mean regression) is based on the idea of
minimizing the Euclidean distance between $y$ and $\hat{y}$. Following
this idea, quantile regression trying to minimizing a so called
$\rho_{\tau}$ 'distance' which is defined as:

$$d_{\tau}(y,\hat{y})=\sum_{i=1}^{n}\rho_{\tau}(y_i-\hat{y})
\enspace \enspace (3)$$

The linear conditional quantile fucntion,
$Q_{Y}(\tau|X=x)=x^{'}_{i}\beta^{'}(\tau)$, can be estimated by
solving

$$\hat{b}_{\tau}=argmin_{b \in
R^{p}}\sum{\rho_{\tau}(y_i-x^{'}b_{\tau})} \enspace \enspace (4)$$

### Observations Used in Quantile Regression Fitting

Computation of quantile regression estimators may be formulated as a
linear programming problem and efficiently solved by simplex or
barrier methods. The former method is used for relative small sample
size $n < 1000$, while the latter can deal with large sample.

#### Simplex Method for Quantile Regression Fitting

As $\hat{y}=X\hat{b_{\tau}}$, when miniming the quantile
regression estimating objective function $(3)$, we get the estimatior
of $b_\tau$. The simplex method
involved linear programming and typically yields a solution at a
vertax. The 'exact-fit' property of simplex regression fitting may
confuse people for considering the reliability of quantile regression
model for its ignoring so many observation. While we should know that
in order to locate the points used for soving the linear equation, all
the observations are involved.

Locating the points used in fitting process  will do great help in
outlier detection. We explore the simplex method, and locate the
observation in function "frame_br", based on which point chart can be
easily drew.

The dataset we use for explain the functions are Australia
Institutions of Sports. This dataset contain 202 observations(102
female and 100 male) and 14 variables. We use this data to explore how
the varibales affect the
BMI of human body. We focus on the female data only.

```{r include = FALSE, message = FALSE}
library(quokar)
library(quantreg)
library(ggplot2)
library(gridExtra)
library(purrr)
library(tidyr)
library(dplyr)
data(ais)
ais_female <- subset(ais, Sex == 0)
tau <- c(0.1, 0.5, 0.9)
br <- rq(BMI ~ LBM, tau = tau, data = ais_female, method = 'br')
coef <- br$coef
br_result <- frame_br(br, tau)
origin_obs <- br_result$data_plot
use_obs <- br_result$choose
ggplot(origin_obs,
    aes(x = value, y = y)) +
    geom_point(alpha = 0.1) +
    geom_abline(slope = coef[2, 1], intercept = coef[1,1]) +
    geom_abline(slope = coef[2, 2], intercept = coef[1,2]) +
    geom_abline(slope = coef[2, 3], intercept = coef[1,3]) +
    ylab('y') +
    xlab('x') +
    facet_wrap(~variable, scales = "free_x", ncol = 2) +
    geom_point(data = use_obs, aes(x = value, y = y,
                                        group = tau_flag,
                                        colour = tau_flag,
                                        shape = obs))
```
#### Interior Point Method for Quantile Regression Fitting

Interior point method is used for large sample size quantile
regression model. Stephen & Roger(1997) introduced interior point
method into quantiler regression model for estimating the
coefficients.

The objective function of quantile regression fitting is

$$min_{b \in R^{p}}\sum_{i=1}^{n}\rho_{\tau}(y_i-x_{i}^{'}b_{\tau})$$

where $\rho_{\tau}(r)=r[\tau-I(r<0])]$ for $\tau \in (0,1)$. This
produce the linear program

$$\min \{\tau e^{'}u+(1-\tau)e^{'}v|y=Xb+u-v, (u,v)\in R_{+}^{2n}\}$$

and the dual formulation is

$$\max\{y^{'}d|X^{'}d=0, d\in [\tau -1, \tau]^{n}\}$$

and, if we setting $a=d+1-\tau$, the formulation will become

$$\max\{y^{'}\} a|X^{'}a=(1-\tau)X^{'}e, a \in [0,1]^{n}}$$

Adding slack variables, $s$, satisfying the constraint $a+s=e$, we
can get the barrier function

$$B(a,s,u)=y^{'}a+\mu \sum_{i=1}^{n}(loga_{i}+logs_{i})$$

We should maximized the barrier function based on the constrains
$X^{'}a=(1-\tau)X^{'}e$ and $a+s=e$. To get the coefficient, the
Newton method will be used. The Newton step $\delta_{a}$ solving

$$max{y^{'}\delta_{a}+\mu
\delta^{'}_{a}(A^{-1}-S^{-1})e-\frac{1}{2}\mu\delta^{'}_{a}(A^{-2}+S^{-2})\delta_{a}}$$

shubject to $X^{'}\delta_{a}=0$, satisfies

$$y+\mu(A^{-1}-S^{-1})e-\mu(A^{-2}+S^{-2})\delta_{a}=Xb$$

It is easy to solve explicitly for the coefficient vector $b$,

$$b=(X^{'}WX)^{-1}X^{'}W[y+\mu(A^{-1}-S^{-1})e]$$

where $W=(A^{-2}+S^{-2})^{-1}$. In each iteration, we set $\mu=0$ and
the basic linear algebra will keep unchange, while the the weighting
matrix $W$ changes.

We use function "frame_fn_obs" to plot the weighted observations used
in model fitting.

```{r include = FALSE}
tau <- c(0.1, 0.5, 0.9)
fn <- rq(BMI ~ LBM + Ht, data = ais_female, tau = tau, method = 'fn')
fn <- frame_fn_obs(fn, tau)
##For tau = 0.1, plot the observations used in quantile regression
##fitting based on interior point method
fn1 <- fn[[1]]
case <- 1: length(fn1)
fn1 <- cbind(case, fn1)
m <- data.frame(y = ais_female$BMI, x1 = ais_female$LBM, x2 =                            ais_female$Ht, fn1)
p <- length(attr(object$coefficients, "dimnames")[[1]])
m_f <- m %>% gather(variable, value, -case, -fn1, -y)
mf_a <- m_f %>%
  group_by(variable) %>%
  arrange(variable, desc(fn1)) %>%
  filter(row_number() %in% 1:p )
p1 <- ggplot(m_f, aes(x = value, y = y)) +
 geom_point(alpha = 0.1) +
  geom_point(data = mf_a, size = 3) +
  facet_wrap(~variable, scale = "free_x")
 ## For tau = 0.5, plot the observations used in quantile regression
 ##fitting based on interior point method
 fn2 <- fn[[2]]
 case <- 1: length(fn2)
 fn2 <- cbind(case, fn2)
 m <- data.frame(y = ais_female$BMI, x1 = ais_female$LBM, 
                 x2 = ais_female$Ht, fn2)
 p <- length(attr(object$coefficients, "dimnames")[[1]])
 m_f <- m %>% gather(variable, value, -case, -fn2, -y)
 mf_a <- m_f %>%
    group_by(variable) %>%
    arrange(variable, desc(fn2)) %>%
    filter(row_number() %in% 1:p )
 p2 <- ggplot(m_f, aes(x = value, y = y)) +
    geom_point(alpha = 0.1) +
    geom_point(data = mf_a, size = 3) +
    facet_wrap(~variable, scale = "free_x")
 ## For tau = 0.9
 fn3 <- fn[[3]]
 case <- 1: length(fn3)
 fn3 <- cbind(case, fn3)
 m <- data.frame(y = ais_female$BMI, x1 = ais_female$LBM, 
                 x2 = ais_female$Ht, fn3)
 p <- length(attr(object$coefficients, "dimnames")[[1]])
 m_f <- m %>% gather(variable, value, -case, -fn3, -y)
 mf_a <- m_f %>%
   group_by(variable) %>%
   arrange(variable, desc(fn3)) %>%
   filter(row_number() %in% 1:p )
 p3 <- ggplot(m_f, aes(x = value, y = y)) +
   geom_point(alpha = 0.1) +
   geom_point(data = mf_a, size = 3) +
   facet_wrap(~variable, scale = "free_x")
 grid.arrange(p1, p2, p3, ncol = 1)
 
##display the weighting matrix
w_obs <- frame_fn_obs(fn, tau)
## For tau=0.1
w1 <- w_obs[[1]]
case <- 1:nrow(w1)
w1 <- cbind(case, w1)
p1 <- ggplot(w1, aes(x = x1, y = y)) +
    geom_point(colour = 'red') +
    geom_text(aes(label = case))
## For tau = 0.5
w2 <- w_obs[[2]]
case <- 1:nrow(w2)
w2 <- cbind(case, w2)
p2 <- ggplot(w2, aes(x = x1, y = y)) +
    geom_point(colour = 'red') +
    geom_text(aes(label = case))
## For tau = 0.9
w3 <- w_obs[[3]]
case <- 1:nrow(w3)
w3 <- cbind(case, w3)
p3 <- ggplot(w3, aes(x = x1, y = y)) +
    geom_point(colour = 'red') +
    geom_text(aes(label = case))
grid.arrange(p1, p2, p3, ncol =3)

##For multi-variables
fn_multi <- rq(BMI ~ LBM + Ht, data = ais_female, tau = tau,
               method = 'fn')
fn_multi$residuals
w_obs <- frame_fn_obs(fn_multi, tau)
head(w_obs)
## For tau=0.1
w1 <- w_obs[[1]]
case <- 1:nrow(w1)
w1 <- cbind(case, w1)
w1_f <- w1 %>% gather(variable, value, -case, -tau_flag, -y)
p1 <- ggplot(w1_f, aes(x = value, y = y)) +
    geom_point(colour = 'red') +
    geom_text(aes(label = case))+
    facet_wrap(~variable, scale = 'free_x')
## For tau = 0.5
w2 <- w_obs[[2]]
case <- 1:nrow(w2)
w2 <- cbind(case, w2)
w2_f <- w2 %>% gather(variable, value, -case, -tau_flag, -y)
p2 <- ggplot(w2_f, aes(x = value, y = y)) +
    geom_point(colour = 'red') +
    geom_text(aes(label = case))+
    facet_wrap(~variable, scale = 'free_x')
## For tau = 0.9
w3 <- w_obs[[3]]
case <- 1:nrow(w3)
w3 <- cbind(case, w3)
w3_f <- w3 %>% gather(variable, value, -case, -tau_flag, -y)
p3 <- ggplot(w3_f, aes(x = value, y = y)) +
    geom_point(colour = 'red') +
    geom_text(aes(label = case))+
    facet_wrap(~variable, scale = 'free_x')
grid.arrange(p1, p2, p3, ncol = 1)

```
we notice that, there are $p$ observations get larger weights than
other observations, and these $p$ sample obvservations are the same
$p$ ones used in simplex method.

In conclusion, the quantile regression fitting process are based on
linear programming. In each fitting process, if we have $p$ parameters
to estimate, $p$ observations will be used or $p$ observations will be
put on larger weights.

### How does outliers affect quantile regression model

Many researchers hold the view that because the the weighted sum of
absolute deviations give a robust measurement of location, the
estimated coefficient vector of quantile regression is not sensitive
to outlier obsercations on the dependent variable(Koenker(1978),
Buchinsky(1998)). And based on the 'exact-fit' property of
coefficients estimating, it is obvious that the quantile regression is
robust comparing to ols regression. However, this conlusion was drawed
on the overall estimations of different quantiles compared to the mean
estimation from OLS regression. If we look at the quantiles
separately, outliers actually will affect the estimation of quantile
regression greatly. Further more, the linear programming method used
for estimating the quantile regression coefficients add difficulties
to figure out how the outliers affect quantile regression model. We
will discuss the role of outliers based on its number, location and
pattern.

#### Outliers simulation study 1: location

 The sample data indicate linear relationship between dependent and
 independent variable with changing slope for different x. We
 condaminate the data with outliers with same pattern while have
 distant location towards the others.

 * Outliers moving in y direction

```{r}
x <- sort(runif(100))
y <- 40*x + x*rnorm(100, 0, 10)
#locate the outliers
selectedX <- sample(50:100,5)
y_location_y1 <- y
y_location_y1[selectedX] <- x[1:5]*rnorm(5, 0, 10)
#keep moving down
y_location_y2 <- y_location_y1
y_location_y2[selectedX] <- y_location_y2[selectedX] - 10
y_location_y3 <- y_location_y2
y_location_y3[selectedX] <- y_location_y3[selectedX] - 10
```

Using the simulated data to construct quantile regression model. By
comparing the four models, we have a brief idea of the effect of
outliers' location. The results show that when outliers moving down in
y direction for 10 unit, it pulls down the slope on every
quantile(comparing the result of model rq(y1~x) and rq(y2
~x)). However, keeping moving down the outliers does no change to the
slopes.

```{r message=FALSE,fig.align="center"}
df <- data.frame(x, y, y_location_y1, y_location_y2, y_location_y3)
df_m <- df %>% gather(variable, value, -x)
ggplot(df_m, aes(x=x, y=value)) +
    geom_point() +
    xlab("x") +
    ylab("y") +
    facet_wrap(~variable, scale = 'free', ncol=2) +
    geom_quantile(quantiles = seq(0.1, 0.9, 0.1))
coefs <- 2:5 %>%
            map(~ rq(df[, .] ~ x, data = df, seq(0.1, 0.9, 0.1))) %>%
            map_df(~ as.data.frame(t(as.matrix(coef(.)))))
tau <- rep(seq(0.1, 0.9, by = 0.1), 4)
model <- paste('rq', rep(1:4, each = 9), sep="")
df_m1 <- data.frame(cbind(model, tau, coefs))
ggplot(df_m1, aes(x = tau, y = x, colour = model)) +
    geom_point() +
    geom_line() +
    xlab('quantile') +
    ylab('coefficients')
```

If moving outliers in same pattern moving on x direction, slopes
change every time outlier moves. To go further, each move does
different effect on different quantiles.

```{r message=FALSE,fig.align="center"}
#locate the outliers
selectedX1 <- sample(50:100, 5)
y_location_x1 <- y
y_location_x1[selectedX1] <- x[1:5]*rnorm(5, 0, 10)
#moving on x direction
selectedX2 <- sample(50:100, 5)
y_location_x2 <- y
y_location_x2[selectedX2] <- x[1:5]*rnorm(5, 0, 10)
selectedX3 <- sample(50:100, 5)
y_location_x3 <- y
y_location_x3[selectedX3] <- x[1:5]*rnorm(5, 0, 10)
df <- data.frame(x, y, y_location_x1, y_location_x2, y_location_x3)
df_m <- df %>% gather(variable, value, -x)
ggplot(df_m, aes(x = x, y=value)) +
    geom_point() +
    xlab("x") +
    ylab("y") +
    facet_wrap(~variable, ncol=2) +
    geom_quantile(quantiles = seq(0.1, 0.9, 0.1))
coefs <- 2:5 %>%
            map(~ rq(df[, .] ~ x, data = df, seq(0.1, 0.9, 0.1))) %>%
            map_df(~ as.data.frame(t(as.matrix(coef(.)))))
tau <- rep(seq(0.1, 0.9, by = 0.1), 4)
model <- paste('rq', rep(1:4, each = 9), sep="")
df_m1 <- data.frame(cbind(model, tau, coefs))
ggplot(df_m1, aes(x = tau, y = x, colour = model)) +
    geom_point() +
    geom_line() +
    xlab('quantile') +
    ylab('coefficients')
```

#### Outliers simulation study 2: Number

As to the 'exact fit' character of estimation process of quantile
regression, the number of outliers should be an important factor
influencing the robustness of the model. Our simulation result proved
this assumption. With growing numbers of outliers, the slopes on each
quantile change.

```{r}
selectedX1 <- sample(50:100, 5)
y_number_y1 <- y
y_number_y1[selectedX1] <- x[1:5]*rnorm(5, 0, 10)
selectedX2 <- sample(50:100, 10)
y_number_y2 <- y
y_number_y2[selectedX2] <- x[1:10]*rnorm(10, 0, 10)
selectedX3 <- sample(50:100, 15)
y_number_y3 <- y
y_number_y3[selectedX3] <- x[1:15]*rnorm(15, 0, 10)
```

```{r message=FALSE,fig.align="center"}
df <- data.frame(x, y, y_number_y1, y_number_y2, y_number_y3)
df_m <- df %>% gather(variable, value, -x)
ggplot(df_m, aes(x=x, y=value)) +
    geom_point() +
    xlab("x") +
    ylab("y") +
    facet_wrap(~variable, ncol=2) +
    geom_quantile(quantiles = seq(0.1, 0.9, 0.1))
coefs <- 2:5 %>%
            map(~ rq(df[, .] ~ x, data = df, seq(0.1, 0.9, 0.1))) %>%
            map_df(~ as.data.frame(t(as.matrix(coef(.)))))
tau <- rep(seq(0.1, 0.9, by = 0.1), 4)
model <- paste('rq', rep(1:4, each = 9), sep="")
df_m1 <- data.frame(cbind(model, tau, coefs))
ggplot(df_m1, aes(x = tau, y = x, colour = model)) +
    geom_point() +
    geom_line() +
    xlab('quantile') +
    ylab('coefficients')
```

#### Outliers simulation study 3: Pattern

Sometimes outliers do have certain patterns, such as sparse or
clustered. We explore how the outlier patterns affect the estimation
of quantile regression model.

* Clustered outliers

```{r}
y_clustered_y1 <- y
y_clustered_y1[96: 100] <- x[1:5]*rnorm(5, 0, 10)
y_clustered_y2 <- y
y_clustered_y2[91: 100] <- x[1:10]*rnorm(10, 0, 10)
y_clustered_y3 <- y
y_clustered_y3[86: 100] <- x[1:15]*rnorm(15, 0, 10)
```

```{r message=FALSE,fig.align="center"}
df <- data.frame(x, y, y_clustered_y1, y_clustered_y2,
                 y_clustered_y3)
df_m <- df %>% gather(variable, value, -x)
ggplot(df_m, aes(x=x, y=value)) +
    geom_point() +
    xlab("x") +
    ylab("y") +
    facet_wrap(~variable, ncol=2) +
    geom_quantile(quantiles = seq(0.1, 0.9, 0.1))
coefs <- 2:5 %>%
            map(~ rq(df[, .] ~ x, data = df, seq(0.1, 0.9, 0.1))) %>%
            map_df(~ as.data.frame(t(as.matrix(coef(.)))))
tau <- rep(seq(0.1, 0.9, by = 0.1), 4)
model <- paste('rq', rep(1:4, each = 9), sep="")
df_m1 <- data.frame(cbind(model, tau, coefs))
ggplot(df_m1, aes(x = tau, y = x, colour = model)) +
    geom_point() +
    geom_line() +
    xlab('quantile') +
    ylab('coefficients')
```

* Sparse outliers

```{r}
y_sparse_y1 <- y
selectedX1 <- sample(50:100, 5, replace = FALSE)
y_sparse_y1[selectedX1] <- x[1:5]*rnorm(5, 0, 10)
y_sparse_y2 <- y
selectedX2 <- sample(50:100, 10, replace = FALSE)
y_sparse_y2[selectedX2] <- x[1:10]*rnorm(10, 0, 10)
y_sparse_y3 <- y
selectedX3 <- sample(50:100, 15, replace = FALSE)
y_sparse_y3[selectedX3] <- x[1:15]*rnorm(15, 0, 10)
```

```{r message=FALSE,fig.align="center"}
df <- data.frame(x, y, y_sparse_y1, y_sparse_y2, y_sparse_y3)
df_m <- df %>% gather(variable, value, -x)
ggplot(df_m, aes(x=x, y=value)) +
    geom_point() +
    xlab("x") +
    ylab("y") +
    facet_wrap(~variable, ncol=2) +
    geom_quantile(quantiles = seq(0.1, 0.9, 0.1))
coefs <- 2:5 %>%
            map(~ rq(df[, .] ~ x, data = df, seq(0.1, 0.9, 0.1))) %>%
            map_df(~ as.data.frame(t(as.matrix(coef(.)))))
tau <- rep(seq(0.1, 0.9, by = 0.1), 4)
model <- paste('rq', rep(1:4, each = 9), sep="")
df_m1 <- data.frame(cbind(model, tau, coefs))
ggplot(df_m1, aes(x = tau, y = x, colour = model)) +
    geom_point() +
    geom_line() +
    xlab('quantile') +
    ylab('coefficients')
```
In conclusion, in terms of each quantile, outliers can have
tremendous influence on estimation results of quantile regression
model. It is necessary to detect them effectively.



### Quantile regression using asymmetric laplace distribution






### Outlier Dignostic Methods for Quantile Regression Model

* Method 1: Studentized Residual

* Method 2: Generalized Cook Distance

* Method 3: Q Function Distance

* Method 4: Posterior Mean Probability

### Simulation Study

* Linear model study


* Non-linear model study

### Application

* Australia Institutions of Sport

* American House Price




